# -*- coding: utf-8 -*-
"""Vector_Space_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MNXlhl1RJdAAbkqDiE57IYfyhyEnIUrg
"""

import os
import string
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Ensure you have the necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Path to the directory containing the text files
text_files = [
    "/content/doc1.txt",
    "/content/doc2.txt",
    "/content/doc3.txt",
    "/content/doc4.txt",
    "/content/doc5.txt"
]

# Step 1: Load the Text Files
documents = []
for file_path in text_files:
    with open(file_path, 'r') as file:
        documents.append(file.read())

# Display loaded documents
print("Loaded Documents:")
for i, doc in enumerate(documents):
    print(f"\nDocument {i+1}:\n{doc}\n{'-'*40}")

# Step 2: Preprocess the Text Data
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)
    # Convert to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove punctuation
    tokens = [token for token in tokens if token not in string.punctuation]
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    # Join tokens back into a single string
    return ' '.join(tokens)

# Apply preprocessing to all documents
processed_documents = [preprocess_text(doc) for doc in documents]

# Display preprocessed documents
print("\nPreprocessed Documents:")
for i, doc in enumerate(processed_documents):
    print(f"\nProcessed Document {i+1}:\n{doc}\n{'-'*40}")

# Step 3: Vectorization Using TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_documents)

# Display the TF-IDF matrix
tfidf_feature_names = vectorizer.get_feature_names_out()
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names, index=[f'Doc{i+1}' for i in range(len(documents))])
print("\nTF-IDF Matrix:")
print(tfidf_df)

# Step 4: Calculate Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix)

# Create a DataFrame for the cosine similarity matrix for better visualization
cosine_sim_df = pd.DataFrame(cosine_sim, index=[f'Doc{i+1}' for i in range(len(documents))], columns=[f'Doc{i+1}' for i in range(len(documents))])

# Display the Cosine Similarity Matrix
print("\nCosine Similarity Matrix:")
print(cosine_sim_df)

# Step 5: Plotting the Similarity Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cosine_sim_df, annot=True, cmap='coolwarm', cbar=True)
plt.title('Cosine Similarity between Documents')
plt.show()

